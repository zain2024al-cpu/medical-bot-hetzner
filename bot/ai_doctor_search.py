# ================================================
# bot/ai_doctor_search.py
# ğŸ¤– Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ - Ù‡Ø¬ÙŠÙ† (Ù…Ù„Ù + AI)
# ================================================

import os
import asyncio
import logging
import re
import time
from functools import lru_cache
from dotenv import load_dotenv

load_dotenv("config.env")
logger = logging.getLogger(__name__)

# âœ… Cache Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©
_last_request_time = 0
_min_request_interval = 1.5  # Ø«Ø§Ù†ÙŠØ© Ø¨ÙŠÙ† ÙƒÙ„ Ø·Ù„Ø¨
_request_cache = {}

# âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„ÙŠØ©
_doctors_db = None


def _normalize_arabic_text(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„Ø¨Ø­Ø«"""
    if not text:
        return ""
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
    text = re.sub(r'[\u064B-\u065F\u0670]', '', text)
    
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
    replacements = {
        'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù±': 'Ø§',
        'Ø©': 'Ù‡',
        'Ù‰': 'ÙŠ',
        'Ø¦': 'ÙŠ', 'Ø¤': 'Ùˆ'
    }
    
    for old, new in replacements.items():
        text = text.replace(old, new)
    
    return text.strip().lower()


def _load_doctors_from_file():
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ù…Ù† Ù…Ù„Ù data/doctors.txt"""
    doctors = []
    file_path = 'data/doctors.txt'
    
    if not os.path.exists(file_path):
        logger.warning(f"âš ï¸ Ù…Ù„Ù Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")
        return doctors
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f, 1):
                line = line.strip()
                
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø³Ø·ÙˆØ± Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
                if not line or line.startswith('#'):
                    continue
                
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø³Ø·Ø±
                parts = line.split('|')
                if len(parts) < 1:
                    continue
                
                name = parts[0].strip()
                hospital = parts[1].strip() if len(parts) > 1 else ""
                department = parts[2].strip() if len(parts) > 2 else ""
                
                if name:
                    doctors.append({
                        'id': f'DB{idx:03d}',
                        'name': name,
                        'hospital': hospital or "Ù…ØªØ§Ø­",
                        'department': department or "Ù…ØªØ§Ø­",
                        'name_normalized': _normalize_arabic_text(name)
                    })
        
        logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(doctors)} Ø·Ø¨ÙŠØ¨ Ù…Ù† Ù…Ù„Ù Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡")
        return doctors
    
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡: {e}")
        return doctors


def get_doctors_database():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ (Ù…Ø¹ cache)"""
    global _doctors_db
    
    if _doctors_db is None:
        _doctors_db = _load_doctors_from_file()
    
    return _doctors_db


def search_doctors_locally(query, hospital=None, department=None):
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù„Ù„Ø£Ø·Ø¨Ø§Ø¡"""
    doctors = get_doctors_database()
    
    if not doctors:
        return []
    
    query_norm = _normalize_arabic_text(query) if query else ""
    hospital_norm = _normalize_arabic_text(hospital) if hospital else ""
    department_norm = _normalize_arabic_text(department) if department else ""
    
    results = []
    
    for doctor in doctors:
        # ÙØ­Øµ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
        if hospital_norm and doctor['hospital'] != "Ù…ØªØ§Ø­":
            doc_hospital_norm = _normalize_arabic_text(doctor['hospital'])
            if hospital_norm not in doc_hospital_norm and doc_hospital_norm not in hospital_norm:
                continue
        
        # ÙØ­Øµ Ø§Ù„Ù‚Ø³Ù…
        if department_norm and doctor['department'] != "Ù…ØªØ§Ø­":
            doc_dept_norm = _normalize_arabic_text(doctor['department'])
            if department_norm not in doc_dept_norm and doc_dept_norm not in department_norm:
                continue
        
        # ÙØ­Øµ Ø§Ù„Ø§Ø³Ù…
        if query_norm:
            if query_norm in doctor['name_normalized']:
                results.append(doctor)
        else:
            # Ø¨Ø¯ÙˆÙ† query - Ø¥Ø±Ø¬Ø§Ø¹ ÙƒÙ„ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚ÙŠÙ† Ù„Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙˆØ§Ù„Ù‚Ø³Ù…
            results.append(doctor)
    
    logger.info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ: ÙˆÙØ¬Ø¯ {len(results)} Ø·Ø¨ÙŠØ¨ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… '{query}'")
    return results


async def get_ai_doctor_suggestions(
    query: str,
    *,
    hospital: str | None = None,
    department: str | None = None,
) -> list:
    """
    ğŸ¤– Ø§Ù‚ØªØ±Ø§Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø£Ø·Ø¨Ø§Ø¡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙˆØ§Ù„Ù‚Ø³Ù….
    
    Args:
        query: Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø¯Ø®Ù„ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠÙƒÙˆÙ† ÙØ§Ø±ØºØ§Ù‹)
        hospital: Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ø®ØªØ§Ø±
        department: Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…/Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ù…Ø®ØªØ§Ø±
    
    Returns:
        list: Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† 5-8 Ø£Ø·Ø¨Ø§Ø¡ Ù…Ù‚ØªØ±Ø­ÙŠÙ†
    """
    global _last_request_time, _request_cache
    
    query = (query or "").strip()
    
    # âœ… Ø­Ø¯ Ø£Ø¯Ù†Ù‰ 3 Ø£Ø­Ø±Ù Ù„Ù„Ø¨Ø­Ø« (Ù„ØªØ¬Ù†Ø¨ Ø·Ù„Ø¨Ø§Øª ÙƒØ«ÙŠØ±Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ÙƒØªØ§Ø¨Ø©)
    if query and len(query) < 3:
        logger.info(f"âš ï¸ Ø§Ù„Ø§Ø³Ù… Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ ({len(query)} Ø£Ø­Ø±Ù) - Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ 3 Ø£Ø­Ø±Ù")
        return []
    
    # Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¨Ø¯ÙˆÙ† query Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ hospital Ùˆ department
    if len(query) < 1 and (not hospital or not department):
        logger.info("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ ÙƒØ§ÙÙ Ù„Ù„Ø¨Ø­Ø«")
        return []

    # âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ cache
    cache_key = f"{query}|{hospital}|{department}"
    
    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù€ cache Ø£ÙˆÙ„Ø§Ù‹
    if cache_key in _request_cache:
        cache_time, cached_results = _request_cache[cache_key]
        # Cache ØµØ§Ù„Ø­ Ù„Ù…Ø¯Ø© 5 Ø¯Ù‚Ø§Ø¦Ù‚
        if time.time() - cache_time < 300:
            logger.info(f"âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… cache Ù„Ù„Ø³ÙŠØ§Ù‚: {cache_key}")
            return cached_results
    
    # âœ… 1ï¸âƒ£ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ù„ÙŠ Ø£ÙˆÙ„Ø§Ù‹ (ÙÙŠ Ù…Ù„Ù Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡)
    local_results = search_doctors_locally(query, hospital, department)
    
    if local_results:
        logger.info(f"âœ… ÙˆÙØ¬Ø¯ {len(local_results)} Ø·Ø¨ÙŠØ¨ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©")
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ cache
        _request_cache[cache_key] = (time.time(), local_results)
        return local_results
    
    # âœ… 2ï¸âƒ£ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    logger.info("ğŸ¤– Ù„Ù… ÙŠÙÙˆØ¬Ø¯ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ù„ÙŠØ© - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...")
    
    # âœ… Rate Limiting - ØªØ¬Ù†Ø¨ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© Ø¬Ø¯Ø§Ù‹
    current_time = time.time()
    time_since_last = current_time - _last_request_time
    
    if time_since_last < _min_request_interval:
        wait_time = _min_request_interval - time_since_last
        logger.info(f"â³ Ø§Ù†ØªØ¸Ø§Ø± {wait_time:.1f}s Ù„ØªØ¬Ù†Ø¨ Too Many Requests")
        await asyncio.sleep(wait_time)
    
    _last_request_time = time.time()

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.warning("âš ï¸ OpenAI API Key ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        return []

    try:
        from openai import OpenAI
    except ImportError:
        logger.error("âŒ OpenAI library not installed")
        return []

    client = OpenAI(api_key=api_key)

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚
    context_parts = []
    if query:
        context_parts.append(f"Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø¯Ø®Ù„: '{query}'")
    if hospital:
        context_parts.append(f"Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰: {hospital}")
    if department:
        context_parts.append(f"Ø§Ù„Ù‚Ø³Ù…/Ø§Ù„ØªØ®ØµØµ: {department}")

    context_text = " | ".join(context_parts) if context_parts else "Ø·Ù„Ø¨ Ø¹Ø§Ù…"

    # Prompt Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª (ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø¯Ø¯ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©)
    prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø·Ø¨ÙŠ Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯. Ø£Ø¹Ø·Ù†ÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù€ 5 Ø£Ø³Ù…Ø§Ø¡ Ø£Ø·Ø¨Ø§Ø¡ Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ† ÙˆÙ…Ù†Ø§Ø³Ø¨ÙŠÙ† Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ:

{context_text}

Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù‡Ù…Ø©:
1. Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆØ´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯ (Ù‡Ù†Ø¯ÙŠØ©ØŒ Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø£Ùˆ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©)
2. ÙƒÙ„ Ø³Ø·Ø±: Ø§Ø³Ù… Ø·Ø¨ÙŠØ¨ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ø¨ØµÙŠØºØ© "Ø¯. Ø§Ù„Ø§Ø³Ù… Ø§Ù„ÙƒØ§Ù…Ù„" Ø£Ùˆ "Dr. Full Name"
3. Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø§Ø³Ù… Ù…Ø¯Ø®Ù„ØŒ Ø§Ù‚ØªØ±Ø­ Ø£Ø³Ù…Ø§Ø¡ Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù‡ Ø£Ùˆ ØªØ­ØªÙˆÙŠÙ‡
4. Ù…ØªÙ†ÙˆØ¹Ø© ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„ØªØ®ØµØµ Ø§Ù„Ù…Ø°ÙƒÙˆØ±
5. Ø¨Ø¯ÙˆÙ† Ø£Ø±Ù‚Ø§Ù… Ø£Ùˆ Ø±Ù…ÙˆØ² - ÙÙ‚Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
6. ÙƒÙ„ Ø§Ø³Ù… ÙÙŠ Ø³Ø·Ø± Ù…Ù†ÙØµÙ„
7. Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­ Ø£Ùˆ Ù†ØµÙˆØµ Ø¥Ø¶Ø§ÙÙŠØ©

Ù…Ø«Ø§Ù„ Ù„Ù„ØµÙŠØºØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©:
Ø¯. Ø£Ù…ÙŠØª ÙƒÙˆÙ…Ø§Ø±
Dr. Rajesh Sharma
Ø¯. Ø³Ø§Ù†Ø¯ÙŠØ§ Ø¨Ø§ØªÙŠÙ„
Dr. Mohammed Ali

Ø§Ù„Ø¢Ù† Ø£Ø¹Ø·Ù†ÙŠ 5 Ø£Ø³Ù…Ø§Ø¡:"""

    def call_openai() -> str:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± Ø·Ø¨ÙŠ ØªÙ‚ØªØ±Ø­ Ø£Ø³Ù…Ø§Ø¡ Ø£Ø·Ø¨Ø§Ø¡ ÙˆØ§Ù‚Ø¹ÙŠØ©. Ø£Ø¹Ø·Ù ÙÙ‚Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ù†Øµ Ø¥Ø¶Ø§ÙÙŠ."
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=150,  # ØªÙ‚Ù„ÙŠÙ„ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            temperature=0.5,  # ØªÙ‚Ù„ÙŠÙ„ Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ù„Ù„Ø¯Ù‚Ø©
        )
        return response.choices[0].message.content.strip()

    try:
        logger.info(f"ğŸ” Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: {context_text}")
        ai_response = await asyncio.to_thread(call_openai)
    except Exception as exc:
        logger.error(f"âŒ AI doctor suggestion failed: {exc}")
        return []

    if not ai_response:
        return []

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ù„Ù‚Ø§Ø¦Ù…Ø© Ø£Ø·Ø¨Ø§Ø¡
    doctors_list = []
    lines = ai_response.strip().split('\n')
    
    for idx, line in enumerate(lines, 1):
        line = line.strip()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø³Ø·Ø± Ù…Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ù„Ø±Ù…ÙˆØ² ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ù…Ø«Ù„: 1. Ø£Ùˆ - Ø£Ùˆ *)
        line = re.sub(r'^[\d\.\-\*\â€¢\â—¦\â†’\â–º]+\s*', '', line)
        
        if not line or len(line) < 3:
            continue
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ "Ø¯." Ø£Ùˆ "Dr."
        if not (line.startswith('Ø¯.') or line.startswith('Dr.') or line.startswith('Ø¯ÙƒØªÙˆØ±')):
            # Ø¥Ø¶Ø§ÙØ© Ø¯. Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø§Ø³Ù… Ø¹Ø±Ø¨ÙŠ
            has_arabic = bool(re.search(r'[\u0600-\u06FF]', line))
            if has_arabic:
                line = f"Ø¯. {line}"
            else:
                line = f"Dr. {line}"
        
        doctors_list.append({
            "id": f"AI{idx:03d}",
            "name": line,
            "hospital": hospital or "Ù…ØªØ§Ø­",
            "department": department or "Ù…ØªØ§Ø­",
        })
        
        # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 8 Ø£Ø·Ø¨Ø§Ø¡
        if len(doctors_list) >= 8:
            break
    
    logger.info(f"âœ… AI Ø§Ù‚ØªØ±Ø­ {len(doctors_list)} Ø·Ø¨ÙŠØ¨ Ù„Ù„Ø³ÙŠØ§Ù‚: {context_text}")
    
    # âœ… Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ cache
    _request_cache[cache_key] = (time.time(), doctors_list)
    
    # âœ… ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù€ cache Ø§Ù„Ù‚Ø¯ÙŠÙ… (Ø£ÙƒØ«Ø± Ù…Ù† 100 Ø¥Ø¯Ø®Ø§Ù„)
    if len(_request_cache) > 100:
        # Ø­Ø°Ù Ø£Ù‚Ø¯Ù… 50 Ø¥Ø¯Ø®Ø§Ù„
        sorted_cache = sorted(_request_cache.items(), key=lambda x: x[1][0])
        for old_key, _ in sorted_cache[:50]:
            del _request_cache[old_key]
    
    return doctors_list

