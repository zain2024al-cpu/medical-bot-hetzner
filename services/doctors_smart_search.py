#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø¹Ù† Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡
ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ - Ø¨Ø­Ø« ÙÙˆØ±ÙŠ Ù…Ø¹ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
Ù†Ø¸Ø§Ù… ØªØ±ØªÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ Ø¯Ø¹Ù… AI
"""

import json
import os
import re
from rapidfuzz import fuzz, process
import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

# Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªÙŠØ±Ø§Ø¯ OpenAI Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙƒÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.info("OpenAI ØºÙŠØ± Ù…ØªØ§Ø­ - Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ ÙÙ‚Ø·")

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“– Ù‚Ø§Ù…ÙˆØ³ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠØ© (Ø¹Ø±Ø¨ÙŠ â†” Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

DEPARTMENT_TRANSLATIONS = {
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…Ø® ÙˆØ§Ù„Ø£Ø¹ØµØ§Ø¨ â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…Ø® ÙˆØ§Ù„Ø£Ø¹ØµØ§Ø¨": ["neurosurgery", "neurological surgery", "brain surgery", "neuro surgery", "adult neurosurgery", "neuro spine"],
    "Ø¬Ø±Ø§Ø­Ø© Ù…Ø® ÙˆØ§Ø¹ØµØ§Ø¨": ["neurosurgery", "neurological surgery", "brain surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…Ø®": ["neurosurgery", "brain surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„ÙÙ‚Ø±ÙŠ": ["spine surgery", "spinal surgery", "neuro spine"],
    
    # â•â•â• Ø§Ù„Ø£Ø¹ØµØ§Ø¨ (Ø·Ø¨) â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø£Ø¹ØµØ§Ø¨": ["neurology", "neurological", "neuro"],
    "Ø§Ù„Ø£Ø¹ØµØ§Ø¨": ["neurology", "neurological"],
    "Ø§Ø¹ØµØ§Ø¨": ["neurology"],
    "Ø§Ù„Ø£Ø¹ØµØ§Ø¨ ÙˆØ§Ù„ØµØ±Ø¹": ["epilepsy", "neurology"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù‚Ù„Ø¨ â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„ØµØ¯Ø±": ["cardiothoracic", "cardiac surgery", "heart surgery", "ctvs"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù‚Ù„Ø¨": ["cardiac surgery", "heart surgery", "cardiothoracic"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ØµØ¯Ø±": ["thoracic surgery", "chest surgery"],
    "Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ù‚Ù„Ø¨": ["heart transplant", "cardiac transplant"],
    
    # â•â•â• Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ â•â•â•
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨": ["cardiology", "cardiac sciences"],
    "Ù‚Ù„Ø¨": ["cardiology", "cardiac"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù‚Ù„Ø¨ Ø§Ù„ØªØ¯Ø§Ø®Ù„ÙŠØ©": ["interventional cardiology"],
    "Ø£Ù…Ø±Ø§Ø¶ Ù‚Ù„Ø¨ Ø§Ù„Ø£Ø·ÙØ§Ù„": ["pediatric cardiology", "paediatric cardiology"],
    "Ø§Ù„ÙÙŠØ²ÙŠÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØ© Ù„Ù„Ù‚Ù„Ø¨": ["electrophysiology", "cardiac electrophysiology"],
    "Ø¹Ù„ÙˆÙ… Ø§Ù„Ù‚Ù„Ø¨": ["cardiac sciences"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¹Ø¸Ø§Ù… â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¹Ø¸Ø§Ù…": ["orthopedic", "orthopaedic", "orthopedics", "bone"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ÙØ§ØµÙ„": ["joint replacement", "joint arthroplasty"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…ÙØ§ØµÙ„": ["joint surgery", "arthroscopy"],
    "ØªÙ†Ø¸ÙŠØ± Ø§Ù„Ù…ÙØ§ØµÙ„": ["arthroscopy"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¥ØµØ§Ø¨Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©": ["sports surgery", "sports injury"],
    "Ø¬Ø±Ø§Ø­Ø© Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø¹Ø¸Ø§Ù…": ["orthopedic oncology", "bone tumor"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ÙŠØ¯": ["hand surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ÙƒØªÙ": ["shoulder surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø±ÙƒØ¨Ø©": ["knee surgery"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…Ø³Ø§Ù„Ùƒ Ø§Ù„Ø¨ÙˆÙ„ÙŠØ© â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ù…Ø³Ø§Ù„Ùƒ Ø§Ù„Ø¨ÙˆÙ„ÙŠØ©": ["urology", "urological"],
    "Ù…Ø³Ø§Ù„Ùƒ Ø¨ÙˆÙ„ÙŠØ©": ["urology", "urological"],
    "Ø¬Ø±Ø§Ø­Ø© Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ù„Ùƒ Ø§Ù„Ø¨ÙˆÙ„ÙŠØ©": ["uro-oncology", "urologic oncology"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¨Ø±ÙˆØ³ØªØ§ØªØ§": ["prostate surgery", "prostate"],
    "Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ÙƒÙ„Ù‰": ["renal transplant", "kidney transplant"],
    
    # â•â•â• Ø§Ù„Ø£ÙˆØ±Ø§Ù… â•â•â•
    "Ø§Ù„Ø£ÙˆØ±Ø§Ù…": ["oncology", "cancer"],
    "Ø§ÙˆØ±Ø§Ù…": ["oncology"],
    "Ø§Ù„Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø·Ø¨ÙŠØ©": ["medical oncology"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø£ÙˆØ±Ø§Ù…": ["surgical oncology"],
    "Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø£ÙˆØ±Ø§Ù… Ø¨Ø§Ù„Ø¥Ø´Ø¹Ø§Ø¹": ["radiation oncology"],
    "Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø¯Ù…": ["hematologic oncology", "hemato-oncology"],
    "Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø±Ø£Ø³ ÙˆØ§Ù„Ø¹Ù†Ù‚": ["head & neck oncology", "head and neck oncology"],
    "Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ù†Ø³Ø§Ø¡": ["gynecologic oncology"],
    "Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ù†Ø§Ø¹ÙŠ Ù„Ù„Ø£ÙˆØ±Ø§Ù…": ["immunotherapy"],
    
    # â•â•â• Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ â•â•â•
    "Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ": ["gastroenterology", "gi"],
    "Ø¬Ù‡Ø§Ø² Ù‡Ø¶Ù…ÙŠ": ["gastroenterology"],
    "Ø§Ù„ÙƒØ¨Ø¯ ÙˆØ§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ": ["hepatogastroenterology", "hepatology"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù‡Ø¶Ù…ÙŠ": ["surgical gastroenterology"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ÙƒØ¨Ø¯": ["hepatology", "liver"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø¹Ø§Ù…Ø© â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø¹Ø§Ù…Ø©": ["general surgery", "surgery", "laparoscopic"],
    "Ø§Ù„Ø¬Ø±Ø§Ø­Ø© Ø·ÙÙŠÙØ© Ø§Ù„ØªÙˆØºÙ„": ["minimally invasive surgery", "laparoscopic"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø³Ù…Ù†Ø©": ["bariatric surgery", "bariatric", "metabolic surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø³Ù…Ù†Ø© ÙˆØ§Ù„Ø£ÙŠØ¶": ["bariatric & metabolic"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø£ÙˆØ¹ÙŠØ© Ø§Ù„Ø¯Ù…ÙˆÙŠØ© â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø£ÙˆØ¹ÙŠØ© Ø§Ù„Ø¯Ù…ÙˆÙŠØ©": ["vascular surgery", "vascular", "endovascular"],
    "Ø§ÙˆØ¹ÙŠØ© Ø¯Ù…ÙˆÙŠØ©": ["vascular"],
    "Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø¯ÙˆØ§Ù„ÙŠ": ["varicose veins"],
    
    # â•â•â• Ø·Ø¨ Ø§Ù„Ø£Ø·ÙØ§Ù„ â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø£Ø·ÙØ§Ù„": ["pediatrics", "paediatrics", "pediatric", "paediatric", "child"],
    "Ø§Ø·ÙØ§Ù„": ["pediatric", "paediatric"],
    "Ø­Ø¯ÙŠØ«ÙŠ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©": ["neonatology", "neonatal"],
    "Ø·ÙˆØ§Ø±Ø¦ Ø§Ù„Ø£Ø·ÙØ§Ù„": ["pediatric emergency", "paediatric emergency"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø£Ø·ÙØ§Ù„": ["pediatric surgery", "paediatric surgery"],
    
    # â•â•â• Ø§Ù„Ù†Ø³Ø§Ø¡ ÙˆØ§Ù„ÙˆÙ„Ø§Ø¯Ø© â•â•â•
    "Ø§Ù„Ù†Ø³Ø§Ø¡ ÙˆØ§Ù„ÙˆÙ„Ø§Ø¯Ø©": ["obstetrics", "gynecology", "obgyn", "obs"],
    "Ù†Ø³Ø§Ø¡ ÙˆÙˆÙ„Ø§Ø¯Ø©": ["obstetrics", "gynecology"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù†Ø³Ø§Ø¡": ["gynecology", "gynaecology"],
    "Ø·Ø¨ Ø§Ù„Ø£Ø¬Ù†Ø©": ["fetal medicine"],
    "Ø§Ù„Ø­Ù…Ù„ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø®Ø·ÙˆØ±Ø©": ["high risk pregnancy"],
    "Ø·Ø¨ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø¨": ["reproductive medicine", "fertility"],
    "Ø§Ù„ÙˆÙ„Ø§Ø¯Ø© Ø§Ù„Ù…Ø¨ÙƒØ±Ø©": ["preterm birth"],
    
    # â•â•â• Ø·Ø¨ Ø§Ù„Ø¹ÙŠÙˆÙ† â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø¹ÙŠÙˆÙ†": ["ophthalmology", "eye"],
    "Ø¹ÙŠÙˆÙ†": ["ophthalmology", "eye"],
    "Ø´Ø¨ÙƒÙŠØ© Ø§Ù„Ø¹ÙŠÙ†": ["retina", "vitreoretinal"],
    "Ø§Ù„Ø¬Ù„ÙˆÙƒÙˆÙ…Ø§": ["glaucoma"],
    "Ø§Ù„Ø³Ø§Ø¯": ["cataract"],
    "Ù‚Ø±Ù†ÙŠØ© Ø§Ù„Ø¹ÙŠÙ†": ["cornea"],
    "Ø¹ÙŠÙˆÙ† Ø§Ù„Ø£Ø·ÙØ§Ù„": ["pediatric ophthalmology"],
    
    # â•â•â• Ø§Ù„Ø£Ù†Ù ÙˆØ§Ù„Ø£Ø°Ù† ÙˆØ§Ù„Ø­Ù†Ø¬Ø±Ø© â•â•â•
    "Ø§Ù„Ø£Ù†Ù ÙˆØ§Ù„Ø£Ø°Ù† ÙˆØ§Ù„Ø­Ù†Ø¬Ø±Ø©": ["ent", "otolaryngology", "ear nose throat", "head & neck", "head and neck"],
    "Ø§Ù„Ø£Ø°Ù† ÙˆØ§Ù„Ø£Ù†Ù ÙˆØ§Ù„Ø­Ù†Ø¬Ø±Ø©": ["ent", "otolaryngology", "ear nose throat", "head & neck", "head and neck"],
    "Ø§Ù†Ù ÙˆØ§Ø°Ù† ÙˆØ­Ù†Ø¬Ø±Ø©": ["ent", "otolaryngology"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø±Ø£Ø³ ÙˆØ§Ù„Ø¹Ù†Ù‚": ["head & neck", "head and neck", "ent", "otolaryngology"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø±Ø£Ø³ ÙˆØ§Ù„Ø¹Ù†Ù‚ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ù…Ø¬Ù…Ø©": ["head & neck", "head and neck", "ent", "otolaryngology", "skull base surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ù…Ø¬Ù…Ø©": ["skull base surgery"],
    "Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ù‚ÙˆÙ‚Ø¹Ø©": ["cochlear implant"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø£Ù†Ù": ["rhinology"],
    "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ø£Ù†Ù Ø§Ù„ØªØ­Ø³Ø³ÙŠ": ["allergic rhinitis"],
    
    # â•â•â• Ø§Ù„Ø·Ø¨ Ø§Ù„Ø¨Ø§Ø·Ù†ÙŠ â•â•â•
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ø¨Ø§Ø·Ù†ÙŠ": ["internal medicine", "general medicine", "medicine"],
    "Ø¨Ø§Ø·Ù†Ø©": ["internal medicine"],
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ø¹Ø§Ù…": ["general medicine"],
    
    # â•â•â• Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ ÙˆØ§Ù„Ø¹Ù†Ø§ÙŠØ© Ø§Ù„Ù…Ø±ÙƒØ²Ø© â•â•â•
    "Ø§Ù„Ø·ÙˆØ§Ø±Ø¦": ["emergency", "er"],
    "Ø·ÙˆØ§Ø±Ø¦": ["emergency"],
    "Ø·Ø¨ Ø§Ù„Ø·ÙˆØ§Ø±Ø¦": ["emergency medicine"],
    "Ø§Ù„Ø¹Ù†Ø§ÙŠØ© Ø§Ù„Ù…Ø±ÙƒØ²Ø©": ["critical care", "intensive care", "icu"],
    "Ø§Ù„Ø¹Ù†Ø§ÙŠØ© Ø§Ù„Ù…Ø´Ø¯Ø¯Ø©": ["intensive care"],
    
    # â•â•â• Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù… â•â•â•
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¯Ù…": ["hematology", "haematology"],
    "Ø²Ø±Ø¹ Ù†Ø®Ø§Ø¹ Ø§Ù„Ø¹Ø¸Ø§Ù…": ["bone marrow transplant", "bmt", "bone marrow"],
    "Ù†Ø®Ø§Ø¹ Ø§Ù„Ø¹Ø¸Ø§Ù…": ["bone marrow"],
    
    # â•â•â• Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø±ÙˆÙ…Ø§ØªÙŠØ²Ù… â•â•â•
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø±ÙˆÙ…Ø§ØªÙŠØ²Ù…": ["rheumatology", "rheumatic"],
    "Ø±ÙˆÙ…Ø§ØªÙŠØ²Ù…": ["rheumatology"],
    "Ø§Ù„Ù…Ù†Ø§Ø¹Ø© Ø§Ù„Ø³Ø±ÙŠØ±ÙŠØ©": ["clinical immunology"],
    "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ù…ÙØ§ØµÙ„": ["arthritis"],
    
    # â•â•â• Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© â•â•â•
    "Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ©": ["dermatology", "skin"],
    "Ø¬Ù„Ø¯ÙŠØ©": ["dermatology"],
    "Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù„Ø¯ÙŠØ© Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠØ©": ["cosmetic dermatology"],
    
    # â•â•â• Ø§Ù„Ø·Ø¨ Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ ÙˆØ§Ù„ØªØ¬Ù…ÙŠÙ„ â•â•â•
    "Ø§Ù„Ø·Ø¨ Ø§Ù„ØªØ¬Ù…ÙŠÙ„ÙŠ": ["aesthetic", "cosmetic", "cosmetic surgery", "aesthetic surgery"],
    "Ø§Ù„ØªØ¬Ù…ÙŠÙ„": ["cosmetology", "cosmetic", "aesthetic", "cosmetic surgery"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ØªØ¬Ù…ÙŠÙ„": ["plastic surgery", "cosmetic surgery", "aesthetic surgery"],
    "Ø·Ø¨ ØªØ¬Ù…ÙŠÙ„ÙŠ": ["cosmetic", "aesthetic"],
    
    # â•â•â• Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ÙƒÙ„Ù‰ â•â•â•
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ÙƒÙ„Ù‰": ["nephrology", "kidney", "renal"],
    "ÙƒÙ„Ù‰": ["nephrology", "kidney"],
    "ØºØ³ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù‰": ["dialysis"],
    
    # â•â•â• Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ØµØ¯Ø± â•â•â•
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ØµØ¯Ø±": ["pulmonology", "chest", "pulmonary"],
    "ØµØ¯Ø±": ["pulmonology", "chest"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ØªÙ†ÙØ³ÙŠ": ["respiratory"],
    "Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø±Ø¦Ø©": ["lung", "pulmonary"],
    "Ø·Ø¨ Ø§Ø¶Ø·Ø±Ø§Ø¨Ø§Øª Ø§Ù„Ù†ÙˆÙ…": ["sleep medicine", "sleep disorder"],
    "Ø§Ù„Ø³Ù„": ["tuberculosis", "tb"],
    
    # â•â•â• Ø§Ù„ØªØ®Ø¯ÙŠØ± â•â•â•
    "Ø§Ù„ØªØ®Ø¯ÙŠØ±": ["anesthesia", "anaesthesia", "anesthesiology"],
    "ØªØ®Ø¯ÙŠØ±": ["anesthesia"],
    "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ù„Ù…": ["pain management", "pain medicine"],
    "Ø·Ø¨ Ø§Ù„Ø£Ù„Ù…": ["pain medicine"],
    
    # â•â•â• Ø§Ù„Ø£Ø´Ø¹Ø© â•â•â•
    "Ø§Ù„Ø£Ø´Ø¹Ø©": ["radiology", "imaging"],
    "Ø§Ø´Ø¹Ø©": ["radiology"],
    "Ø§Ù„ØªØµÙˆÙŠØ± Ø§Ù„Ø·Ø¨ÙŠ": ["imaging", "diagnostic radiology"],
    "Ø§Ù„Ø£Ø´Ø¹Ø© Ø§Ù„ØªØ¯Ø§Ø®Ù„ÙŠØ©": ["interventional radiology"],
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ù†ÙˆÙˆÙŠ": ["nuclear medicine"],
    
    # â•â•â• Ø±Ø¹Ø§ÙŠØ© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø³ÙƒØ±ÙŠ â•â•â•
    "Ø±Ø¹Ø§ÙŠØ© Ø§Ù„Ù‚Ø¯Ù… Ø§Ù„Ø³ÙƒØ±ÙŠ": ["diabetic foot", "diabetic foot care"],
    "Ù‚Ø¯Ù… Ø³ÙƒØ±ÙŠ": ["diabetic foot"],
    "Ø·Ø¨ Ø§Ù„Ù‚Ø¯Ù…": ["podiatry"],
    
    # â•â•â• Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù†": ["dental", "dentistry", "oral"],
    "Ø§Ø³Ù†Ø§Ù†": ["dental", "dentistry"],
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„ÙÙ… ÙˆØ§Ù„ÙÙƒÙŠÙ†": ["maxillofacial", "oral surgery"],
    "ØªÙ‚ÙˆÙŠÙ… Ø§Ù„Ø£Ø³Ù†Ø§Ù†": ["orthodontics"],
    
    # â•â•â• Ø§Ù„ØºØ¯Ø¯ Ø§Ù„ØµÙ…Ø§Ø¡ â•â•â•
    "Ø§Ù„ØºØ¯Ø¯ Ø§Ù„ØµÙ…Ø§Ø¡": ["endocrinology", "endocrine"],
    "ØºØ¯Ø¯ ØµÙ…Ø§Ø¡": ["endocrinology"],
    "Ø§Ù„Ø³ÙƒØ±ÙŠ": ["diabetes", "diabetic"],
    "Ø§Ù„ØºØ¯Ø© Ø§Ù„Ø¯Ø±Ù‚ÙŠØ©": ["thyroid"],
    
    # â•â•â• Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ â•â•â•
    "Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ": ["physiotherapy", "physical therapy"],
    "Ø¹Ù„Ø§Ø¬ Ø·Ø¨ÙŠØ¹ÙŠ": ["physiotherapy"],
    "Ø§Ù„Ø·Ø¨ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠ": ["physical medicine", "pmr"],
    "Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ£Ù‡ÙŠÙ„": ["rehabilitation"],
    "Ø§Ù„Ø·Ø¨ Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ£Ù‡ÙŠÙ„": ["physical medicine & rehabilitation", "pmr"],
    
    # â•â•â• Ø§Ù„Ø·Ø¨ Ø§Ù„Ù†ÙØ³ÙŠ â•â•â•
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ù†ÙØ³ÙŠ": ["psychiatry", "psychiatric"],
    "Ù†ÙØ³ÙŠØ©": ["psychiatry", "mental health"],
    "Ø§Ù„ØµØ­Ø© Ø§Ù„Ù†ÙØ³ÙŠØ©": ["mental health"],
    "Ø¹Ù„Ù… Ø§Ù„Ù†ÙØ³": ["psychology"],
    "Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ©": ["counseling"],
    
    # â•â•â• Ø§Ù„ØªØºØ°ÙŠØ© â•â•â•
    "Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø¹Ù„Ø§Ø¬ÙŠØ©": ["nutrition", "dietitian", "clinical nutrition"],
    "ØªØºØ°ÙŠØ©": ["nutrition"],
    
    # â•â•â• Ø§Ù„Ø·Ø¨ Ø§Ù„Ù…Ø®Ø¨Ø±ÙŠ â•â•â•
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ù…Ø®Ø¨Ø±ÙŠ": ["laboratory medicine", "lab medicine", "pathology"],
    "Ø¹Ù„Ù… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆØ¨Ø§Øª": ["microbiology"],
    "Ø¹Ù„Ù… Ø§Ù„Ø£Ù…Ø±Ø§Ø¶": ["pathology"],
    "Ø¹Ù„Ù… Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø³Ø±ÙŠØ±ÙŠ": ["clinical pathology"],
    
    # â•â•â• Ø·Ø¨ Ø§Ù„Ø£Ø³Ø±Ø© â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø£Ø³Ø±Ø©": ["family medicine", "general practice", "gp"],
    
    # â•â•â• Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¹Ø¯ÙŠØ© â•â•â•
    "Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¹Ø¯ÙŠØ©": ["infectious disease", "infection"],
    
    # â•â•â• Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø«Ø¯ÙŠ â•â•â•
    "Ø¬Ø±Ø§Ø­Ø© Ø§Ù„Ø«Ø¯ÙŠ": ["breast surgery", "breast", "mammology"],
    "Ø£ÙˆØ±Ø§Ù… Ø§Ù„Ø«Ø¯ÙŠ": ["breast oncology"],
    
    # â•â•â• Ø·Ø¨ Ø§Ù„Ø´ÙŠØ®ÙˆØ®Ø© â•â•â•
    "Ø·Ø¨ Ø§Ù„Ø´ÙŠØ®ÙˆØ®Ø©": ["geriatrics", "geriatric", "elderly care"],
    
    # â•â•â• Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ© â•â•â•
    "Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©": ["genetics", "genetic"],
    "Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ø§Øª Ø§Ù„ÙˆØ±Ø§Ø«ÙŠØ©": ["genetic counseling"],
    
    # â•â•â• Ø§Ù„Ø·Ø¨ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ â•â•â•
    "Ø§Ù„Ø·Ø¨ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ": ["sports medicine", "sports injury"],
    
    # â•â•â• Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡ â•â•â•
    "Ø²Ø±Ø§Ø¹Ø© Ø§Ù„Ø£Ø¹Ø¶Ø§Ø¡": ["transplant", "organ transplant"],
    "Ø²Ø±Ø§Ø¹Ø© Ø§Ù„ÙƒØ¨Ø¯": ["liver transplant"],
    
    # â•â•â• Ø§Ù„ØªÙˆÙ„ÙŠØ¯ â•â•â•
    "Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©": ["obstetrics", "childbirth"],
    "Ø§Ù„Ø­Ù…Ù„": ["pregnancy", "antenatal"],
    
    # â•â•â• Ø¹Ø§Ù… â•â•â•
    "ØªØ®ØµØµ Ø¹Ø§Ù…": ["general", "consultant"],
}

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ“š ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

_doctors_cache = None

def load_doctors():
    """
    ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…
    
    ÙŠØ­Ø§ÙˆÙ„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù… Ø£ÙˆÙ„Ø§Ù‹ (doctors_organized.json)
    Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ØŒ ÙŠØ­Ù…Ù„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù‚Ø¯ÙŠÙ… (doctors_database.json)
    """
    global _doctors_cache
    
    if _doctors_cache is not None:
        return _doctors_cache
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù… Ø£ÙˆÙ„Ø§Ù‹
    organized_path = 'data/doctors_organized.json'
    old_path = 'data/doctors_database.json'
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…
        if os.path.exists(organized_path):
            with open(organized_path, 'r', encoding='utf-8') as f:
                organized_data = json.load(f)
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ù†Ø¸Ù…Ø© Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ø³Ø·Ø­Ø© Ù„Ù„Ø£Ø·Ø¨Ø§Ø¡
            doctors_list = []
            hospitals = organized_data.get('hospitals', {})
            
            for hospital_name, departments in hospitals.items():
                for dept_key, dept_data in departments.items():
                    dept_ar = dept_data.get('department_ar', '')
                    dept_en = dept_data.get('department_en', '')
                    doctors = dept_data.get('doctors', [])
                    
                    for doctor_name in doctors:
                        doctors_list.append({
                            'name': doctor_name,
                            'hospital': hospital_name,
                            'department_ar': dept_ar,
                            'department_en': dept_en,
                            'department': dept_key  # Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ÙÙ‚
                        })
            
            _doctors_cache = doctors_list
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(_doctors_cache)} Ø·Ø¨ÙŠØ¨ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…")
            return _doctors_cache
        
        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ù†Ø¸Ù…ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù‚Ø¯ÙŠÙ…
        elif os.path.exists(old_path):
            with open(old_path, 'r', encoding='utf-8') as f:
                _doctors_cache = json.load(f)
            logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(_doctors_cache)} Ø·Ø¨ÙŠØ¨ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù‚Ø¯ÙŠÙ…")
            return _doctors_cache
        else:
            logger.warning("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù Ø£Ø·Ø¨Ø§Ø¡ (Ù„Ø§ Ø§Ù„Ù…Ù†Ø¸Ù… ÙˆÙ„Ø§ Ø§Ù„Ù‚Ø¯ÙŠÙ…)")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡: {e}")
        return []


def reload_doctors():
    """Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©"""
    global _doctors_cache
    _doctors_cache = None
    return load_doctors()


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _clean_doctor_name(name):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ø³Ù… Ø§Ù„Ø·Ø¨ÙŠØ¨ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù„Ù‚Ø§Ø¨ ÙˆØ§Ù„Ø±Ù…ÙˆØ²"""
    if not name:
        return ""
    
    # ØªØ·Ø¨ÙŠØ¹ Ø´Ø§Ù…Ù„
    cleaned = name.lower().strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù„Ù‚Ø§Ø¨ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
    prefixes = [
        'dr.', 'dr ', 'doctor', 'doctors', 'Ø¯.', 'Ø¯ÙƒØªÙˆØ±', 'Ø¯ÙƒØªÙˆØ±Ù‡',
        'prof.', 'prof ', 'professor', 'professors', 'Ø£Ø³ØªØ§Ø°', 'Ø£Ø³ØªØ§Ø°Ø©',
        'mr.', 'mr ', 'mrs.', 'mrs ', 'ms.', 'ms ',
        'sir', 'sir ', 'miss', 'miss '
    ]
    for prefix in prefixes:
        cleaned = cleaned.replace(prefix, ' ')
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ø£Ù‚ÙˆØ§Ø³
    cleaned = cleaned.replace('(', ' ').replace(')', ' ')
    cleaned = cleaned.replace('[', ' ').replace(']', ' ')
    cleaned = cleaned.replace('.', ' ').replace(',', ' ')
    cleaned = cleaned.replace('-', ' ').replace('_', ' ')
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned


def _get_name_signature(name_clean):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆÙ‚ÙŠØ¹ ÙØ±ÙŠØ¯ Ù„Ù„Ø§Ø³Ù… (Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ† + Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‚ØµÙŠØ±Ø©)"""
    words = [w for w in name_clean.split() if len(w) > 0]
    
    if not words:
        return ""
    
    if len(words) >= 2:
        # Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ†
        signature = f"{words[0]} {words[1]}"
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ù‚ØµÙŠØ±Ø© (Ø­Ø±Ù Ø£Ùˆ Ø­Ø±ÙÙŠÙ†)ØŒ Ø£Ø¶ÙÙ‡Ø§ Ù„Ù„ØªÙˆÙ‚ÙŠØ¹
        if len(words) >= 3 and len(words[2]) <= 2:
            signature += f" {words[2]}"
        return signature
    else:
        return words[0]


def _remove_duplicate_doctors(doctors_list):
    """
    Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ÙƒØ±Ø±ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ - Ù…Ø­Ø³Ù‘Ù†Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„
    
    Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©:
    1. ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„ Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ (Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù„Ù‚Ø§Ø¨ ÙˆØ§Ù„Ø±Ù…ÙˆØ²)
    2. Ù…Ù‚Ø§Ø±Ù†Ø© fuzzy Ø¹Ø§Ù„ÙŠØ© (95%+) Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒØ§Ù…Ù„Ø©
    3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ† + Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‚ØµÙŠØ±Ø©
    4. ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø£Ø·ÙˆÙ„/Ø§Ù„Ø£ÙƒØ«Ø± Ø§ÙƒØªÙ…Ø§Ù„Ø§Ù‹
    """
    if not doctors_list:
        return doctors_list
    
    # Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„ÙØ±ÙŠØ¯ÙŠÙ† Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙ‡Ù…
    unique_doctors = []
    seen_info = []  # [(doctor, name_clean, original_name, name_words)]
    
    for doctor in doctors_list:
        original_name = doctor.get('name', '').strip()
        if not original_name:
            continue
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³Ù…
        name_clean = _clean_doctor_name(original_name)
        if not name_clean:
            continue
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø§Ø³Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        name_words = [w for w in name_clean.split() if len(w) > 0]
        if len(name_words) < 1:
            continue
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ†
        is_duplicate = False
        duplicate_index = None
        
        for idx, (existing_doctor, existing_clean, existing_original, existing_words) in enumerate(seen_info):
            # 1. Ù…Ù‚Ø§Ø±Ù†Ø© fuzzy Ø¹Ø§Ù„ÙŠØ© Ù„Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒØ§Ù…Ù„Ø© (95%+)
            similarity = fuzz.ratio(name_clean, existing_clean)
            if similarity >= 95:
                is_duplicate = True
                duplicate_index = idx
                break
            
            # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ†
            if len(name_words) >= 2 and len(existing_words) >= 2:
                if name_words[0] == existing_words[0] and name_words[1] == existing_words[1]:
                    # Ø¥Ø°Ø§ ÙƒØ§Ù†Ø§ Ù„Ù‡Ù…Ø§ Ù†ÙØ³ Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ†
                    if len(name_words) == 2 and len(existing_words) == 2:
                        # ÙƒÙ„Ù…ØªÙŠÙ† ÙÙ‚Ø· - Ù…ØªØ·Ø§Ø¨Ù‚Ø§Ù†
                        is_duplicate = True
                        duplicate_index = idx
                        break
                    elif len(name_words) >= 3 and len(existing_words) >= 3:
                        # ÙƒÙ„Ø§Ù‡Ù…Ø§ Ù„Ù‡ ÙƒÙ„Ù…Ø© Ø«Ø§Ù„Ø«Ø©
                        third_word = name_words[2]
                        existing_third = existing_words[2]
                        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø© Ù‚ØµÙŠØ±Ø© (Ø­Ø±Ù Ø£Ùˆ Ø­Ø±ÙÙŠÙ†) Ø£Ùˆ Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ø¬Ø¯Ø§Ù‹
                        if len(third_word) <= 2 and len(existing_third) <= 2:
                            is_duplicate = True
                            duplicate_index = idx
                            break
                        elif fuzz.ratio(third_word, existing_third) >= 90:
                            is_duplicate = True
                            duplicate_index = idx
                            break
                    elif len(name_words) == 2 and len(existing_words) >= 3:
                        # Ø£Ø­Ø¯Ù‡Ù…Ø§ Ø¨Ø¯ÙˆÙ† ÙƒÙ„Ù…Ø© Ø«Ø§Ù„Ø«Ø© ÙˆØ§Ù„Ø¢Ø®Ø± Ø¨Ù‡
                        if len(existing_words[2]) <= 2:
                            is_duplicate = True
                            duplicate_index = idx
                            break
                    elif len(name_words) >= 3 and len(existing_words) == 2:
                        # Ø£Ø­Ø¯Ù‡Ù…Ø§ Ø¨Ø¯ÙˆÙ† ÙƒÙ„Ù…Ø© Ø«Ø§Ù„Ø«Ø© ÙˆØ§Ù„Ø¢Ø®Ø± Ø¨Ù‡
                        if len(name_words[2]) <= 2:
                            is_duplicate = True
                            duplicate_index = idx
                            break
            
            # 3. Ù…Ù‚Ø§Ø±Ù†Ø© fuzzy Ù…ØªÙˆØ³Ø·Ø© (90%+) Ù…Ø¹ ØªØ­Ù‚Ù‚ Ø¥Ø¶Ø§ÙÙŠ Ù…Ù† Ø£ÙˆÙ„ ÙƒÙ„Ù…ØªÙŠÙ†
            elif similarity >= 90:
                if len(name_words) >= 2 and len(existing_words) >= 2:
                    if name_words[0] == existing_words[0] and name_words[1] == existing_words[1]:
                        is_duplicate = True
                        duplicate_index = idx
                        break
        
        if is_duplicate:
            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„
            existing_doctor, existing_clean, existing_original, existing_words = seen_info[duplicate_index]
            
            # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø£Ø·ÙˆÙ„ Ø£Ùˆ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "Prof"
            current_has_prof = 'prof' in original_name.lower()
            existing_has_prof = 'prof' in existing_original.lower()
            
            should_replace = False
            if current_has_prof and not existing_has_prof:
                should_replace = True
            elif len(original_name) > len(existing_original) and not (existing_has_prof and not current_has_prof):
                should_replace = True
            elif len(name_words) > len(existing_words):
                # ØªÙØ¶ÙŠÙ„ Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø£ÙƒØ«Ø± (Ø£ÙƒØ«Ø± Ø§ÙƒØªÙ…Ø§Ù„Ø§Ù‹)
                should_replace = True
            
            if should_replace:
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø§Ù„Ù‚Ø¯ÙŠÙ…
                unique_doctors.remove(existing_doctor)
                seen_info[duplicate_index] = (doctor, name_clean, original_name, name_words)
                unique_doctors.append(doctor)
                logger.debug(f"   ğŸ”„ Ø§Ø³ØªØ¨Ø¯Ø§Ù„: {existing_original} Ø¨Ù€ {original_name} (similarity: {similarity}%)")
            else:
                logger.debug(f"   ğŸ”„ ØªÙƒØ±Ø§Ø± Ù…Ø­Ø°ÙˆÙ: {original_name} (Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù€ {existing_original}, similarity: {similarity}%)")
        else:
            # Ø·Ø¨ÙŠØ¨ Ø¬Ø¯ÙŠØ¯ - Ø¥Ø¶Ø§ÙØªÙ‡
            unique_doctors.append(doctor)
            seen_info.append((doctor, name_clean, original_name, name_words))
    
    if len(unique_doctors) < len(doctors_list):
        removed_count = len(doctors_list) - len(unique_doctors)
        logger.info(f"   ğŸ”„ ØªÙ… Ø¥Ø²Ø§Ù„Ø© {removed_count} Ø·Ø¨ÙŠØ¨ Ù…ÙƒØ±Ø± Ù…Ù† {len(doctors_list)} Ø·Ø¨ÙŠØ¨")
    
    return unique_doctors

def normalize_text(text):
    """ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù†Øµ Ù„Ù„Ø¨Ø­Ø«"""
    if not text:
        return ""
    
    text = text.lower().strip()
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù„Ù‚Ø§Ø¨
    text = text.replace('dr.', '').replace('dr ', '').replace('Ø¯.', '').replace('Ø¯ÙƒØªÙˆØ±', '')
    text = text.replace('prof.', '').replace('prof ', '').replace('Ø£Ø³ØªØ§Ø°', '')
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§ÙØ§Øª
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def find_department_english_terms(arabic_dept):
    """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„Ø© Ù„Ù‚Ø³Ù… Ø¹Ø±Ø¨ÙŠ"""
    if not arabic_dept:
        return []
    
    arabic_normalized = normalize_text(arabic_dept)
    english_terms = []
    
    for ar_key, en_values in DEPARTMENT_TRANSLATIONS.items():
        ar_key_normalized = normalize_text(ar_key)
        # Ø¥Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³
        if ar_key_normalized in arabic_normalized or arabic_normalized in ar_key_normalized:
            english_terms.extend(en_values)
    
    return english_terms


def search_doctors(query, hospital=None, department=None, specialty_type=None, limit=10):
    """
    Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø·Ø¨Ø§Ø¡ Ù…Ø¹ ÙÙ„ØªØ±Ø©
    
    Args:
        query: Ù†Øµ Ø§Ù„Ø¨Ø­Ø«
        hospital: Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ù„ÙÙ„ØªØ±Ø©)
        department: Ø§Ù„Ù‚Ø³Ù… (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ù„Ù„ÙÙ„ØªØ±Ø©)
        specialty_type: Ù†ÙˆØ¹ Ø§Ù„ØªØ®ØµØµ - "medical" (Ø¨Ø§Ø·Ù†ÙŠ) Ø£Ùˆ "surgical" (Ø¬Ø±Ø§Ø­ÙŠ) (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        limit: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ø§ÙØªØ±Ø§Ø¶ÙŠ 10)
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…Ù‚ØªØ±Ø­ÙŠÙ†
    """
    
    doctors = load_doctors()
    
    if not doctors:
        logger.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø·Ø¨Ø§Ø¡ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©")
        return []
    
    logger.info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« - Query: '{query}' | Hospital: '{hospital}' | Dept: '{department}'")
    
    # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙˆØ§Ù„Ù‚Ø³Ù…
    filtered = doctors
    
    if hospital:
        hospital_normalized = normalize_text(hospital)
        
        # ÙÙ„ØªØ±Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ù…Ø³ØªØ´ÙÙ‰ - ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªØ´ÙÙŠØ§Øª
        filtered_by_hospital = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ (Ù…Ø«Ù„ CMI, RV, Whitefield, Old Airport Road)
        hospital_words = hospital_normalized.split()
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© (Ù„ÙŠØ³Øª ÙƒÙ„Ù…Ø§Øª Ø¹Ø§Ù…Ø© Ù…Ø«Ù„ "hospital", "medical", "center")
        common_words = {'hospital', 'medical', 'center', 'clinic', 'healthcare', 'health', 'care', 'institute', 'institution'}
        distinctive_words = [w for w in hospital_words if w not in common_words and len(w) >= 2]
        
        for d in filtered:
            doc_hospital = normalize_text(d.get('hospital', ''))
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 1: ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ 100% (Ø§Ù„Ø£ÙØ¶Ù„)
            if hospital_normalized == doc_hospital:
                filtered_by_hospital.append(d)
                continue
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 2: ØªØ·Ø§Ø¨Ù‚ Ø¬Ø²Ø¦ÙŠ - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø®Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Ø£Ùˆ Ø§Ù„Ø¹ÙƒØ³)
            # Ù…Ø«Ø§Ù„: "Aster Whitefield" ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ·Ø§Ø¨Ù‚ "Aster Whitefield Hospital, Bangalore"
            if hospital_normalized in doc_hospital or doc_hospital in hospital_normalized:
                filtered_by_hospital.append(d)
                continue
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 3: ØªØ·Ø§Ø¨Ù‚ fuzzy Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ (90%+)
            hospital_match_ratio = fuzz.ratio(hospital_normalized, doc_hospital)
            if hospital_match_ratio >= 90:
                filtered_by_hospital.append(d)
                continue
            
            # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© 4: ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© - ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©
            if len(distinctive_words) > 0:
                doc_hospital_words = doc_hospital.split()
                doc_distinctive_words = [w for w in doc_hospital_words if w not in common_words and len(w) >= 2]
                
                # ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                all_distinctive_match = True
                for h_word in distinctive_words:
                    found_match = False
                    for d_word in doc_distinctive_words:
                        # ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ Ø£Ùˆ fuzzy Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ (90%+)
                        if h_word == d_word or fuzz.ratio(h_word, d_word) >= 90:
                            found_match = True
                            break
                    if not found_match:
                        all_distinctive_match = False
                        break
                
                # Ø¥Ø°Ø§ ØªØ·Ø§Ø¨Ù‚Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù…ÙŠØ²Ø©ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ·Ø§Ø¨Ù‚ Ø¹Ø§Ù„ÙŠ Ù„Ù„Ø§Ø³Ù… Ø§Ù„ÙƒØ§Ù…Ù„
                if all_distinctive_match:
                    if hospital_match_ratio >= 80:
                        filtered_by_hospital.append(d)
            else:
                # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ù…ÙŠØ²Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ fuzzy Ø¹Ø§Ù„ÙŠ (90%+)
                if hospital_match_ratio >= 90:
                    filtered_by_hospital.append(d)
        
        filtered = filtered_by_hospital
        logger.info(f"   Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ '{hospital}': {len(filtered)} Ø·Ø¨ÙŠØ¨")
    
    # Ø­ÙØ¸ Ù†ØªÙŠØ¬Ø© ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ Ù„Ù„Ù€ fallback
    hospital_filtered = filtered.copy()
    
    if department:
        dept_normalized = normalize_text(department)
        
        # âœ… Ø¥Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© "Ø¹Ø±Ø¨ÙŠ | Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ"ØŒ Ø§ÙØµÙ„Ù‡Ù…Ø§
        dept_ar, dept_en = None, None
        if '|' in department:
            parts = department.split('|')
            if len(parts) >= 2:
                dept_ar = normalize_text(parts[0])
                dept_en = normalize_text(parts[1])
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©
        dept_keywords = [w for w in dept_normalized.split() if len(w) > 2]
        
        # ÙÙ„ØªØ±Ø© Ø¯Ù‚ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹
        filtered_by_dept = []
        for d in filtered:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ 3 Ø­Ù‚ÙˆÙ„: department, department_ar, department_en
            doc_dept = normalize_text(d.get('department', ''))
            doc_dept_ar = normalize_text(d.get('department_ar', ''))
            doc_dept_en = normalize_text(d.get('department_en', ''))
            
            # Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ù„Ù„Ø¨Ø­Ø«
            all_dept_text = f"{doc_dept} {doc_dept_ar} {doc_dept_en}".lower()
            
            if not all_dept_text.strip():
                continue
            
            # âœ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ ØµØ±ÙŠØ­ Ù„Ù€ "dentistry" Ùˆ "dental" Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ENT
            # Ù‡Ø°Ø§ Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ø£Ù† "dentistry" ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "ent" ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø©
            if 'ent' in dept_normalized or 'Ø§Ø°Ù†' in dept_normalized or 'Ø§Ù†Ù' in dept_normalized or 'Ø­Ù†Ø¬Ø±Ø©' in dept_normalized:
                doc_dept_lower = doc_dept_en.lower()
                if 'dentistry' in doc_dept_lower or 'dental' in doc_dept_lower:
                    # Ù‡Ø°Ø§ Ù‚Ø³Ù… Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† - Ù„Ø§ Ù†Ø·Ø§Ø¨Ù‚Ù‡ Ù…Ø¹ ENT
                    logger.debug(f"      âŒ ØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ {d.get('name', '')} - Ù‚Ø³Ù… Ø·Ø¨ Ø§Ù„Ø£Ø³Ù†Ø§Ù† (Dentistry/Dental)")
                    continue
            
            match_found = False
            
            # âœ… Ø·Ø±ÙŠÙ‚Ø© 1: ØªØ·Ø§Ø¨Ù‚ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ù„ØºØ© (Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ - Ø£ÙˆÙ„ÙˆÙŠØ© Ù‚ØµÙˆÙ‰)
            if dept_ar and dept_en:
                # ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ù„Ù‚Ø³Ù… Ù…Ø·Ø§Ø¨Ù‚ ØªÙ…Ø§Ù…Ø§Ù‹ Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
                dept_ar_words = set(dept_ar.split())
                dept_en_words = set(dept_en.split())
                
                doc_dept_ar_words = set(doc_dept_ar.split())
                doc_dept_en_words = set(doc_dept_en.split())
                doc_dept_words = set(doc_dept.split())
                
                # ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚: ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø©
                # Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 70% Ù…Ù† ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
                ar_match_ratio = len(dept_ar_words & doc_dept_ar_words) / len(dept_ar_words) if dept_ar_words else 0
                en_match_ratio = len(dept_en_words & doc_dept_en_words) / len(dept_en_words) if dept_en_words else 0
                dept_match_ratio = len(dept_en_words & doc_dept_words) / len(dept_en_words) if dept_en_words else 0
                
                # ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚: ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ ØªØ·Ø§Ø¨Ù‚ Ù‚ÙˆÙŠ (70% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)
                if ar_match_ratio >= 0.7 or en_match_ratio >= 0.7 or dept_match_ratio >= 0.7:
                    match_found = True
                    logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ Ø«Ù†Ø§Ø¦ÙŠ Ø¯Ù‚ÙŠÙ‚: {d.get('name', '')} (AR: {ar_match_ratio:.2f}, EN: {en_match_ratio:.2f})")
            
            # Ø·Ø±ÙŠÙ‚Ø© 2: ØªØ·Ø§Ø¨Ù‚ Ù…Ø¨Ø§Ø´Ø± Ø¯Ù‚ÙŠÙ‚ (ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø© ÙÙ‚Ø·)
            if not match_found:
                # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
                dept_words = set(dept_normalized.split())
                # ØªÙ‚Ø³ÙŠÙ… Ù‚Ø³Ù… Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
                doc_words = set(all_dept_text.split())
                
                # ØªØ·Ø§Ø¨Ù‚: ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© (ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø©)
                if dept_words and dept_words.issubset(doc_words):
                    match_found = True
                    logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø©: {d.get('name', '')}")
            
            # Ø·Ø±ÙŠÙ‚Ø© 3: Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ (Ø¹Ø±Ø¨ÙŠ â†’ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ) - ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø© ÙÙ‚Ø·
            if not match_found:
                english_terms = find_department_english_terms(department)
                if english_terms:
                    for term in english_terms:
                        term_normalized = normalize_text(term)
                        term_words = set(term_normalized.split())
                        
                        # ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø© ÙÙ‚Ø·
                        doc_dept_en_words = set(doc_dept_en.split())
                        all_dept_words = set(all_dept_text.split())
                        
                        # ØªØ·Ø§Ø¨Ù‚ Ù…Ø­Ø³Ù‘Ù†: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒÙ„Ù…Ø© "ent" Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù†
                        if 'ent' in term_normalized:
                            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† "ent" ÙƒÙƒÙ„Ù…Ø© ÙƒØ§Ù…Ù„Ø© (ÙˆÙ„ÙŠØ³ Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙƒÙ„Ù…Ø© Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ "dentistry")
                            # (Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† dentistry ØªÙ… Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø­Ù„Ù‚Ø©)
                            ent_pattern = r'\bent\b'
                            if (re.search(ent_pattern, doc_dept_en.lower(), re.IGNORECASE) or 
                                re.search(ent_pattern, all_dept_text.lower(), re.IGNORECASE) or
                                'otolaryngology' in doc_dept_en.lower()):
                                match_found = True
                                logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ Ù‚Ø§Ù…ÙˆØ³ (ENT): {d.get('name', '')}")
                                break
                        
                        if term_words.issubset(doc_dept_en_words) or term_words.issubset(all_dept_words):
                            match_found = True
                            logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ Ù‚Ø§Ù…ÙˆØ³: {d.get('name', '')}")
                            break
            
            # Ø·Ø±ÙŠÙ‚Ø© 4: ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (ÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø© ÙÙ‚Ø·)
            if not match_found and dept_keywords:
                # ØªÙ‚Ø³ÙŠÙ… Ù†Øµ Ø§Ù„Ù‚Ø³Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
                all_dept_words = set(all_dept_text.split())
                dept_keywords_set = set(dept_keywords)
                
                # ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙƒÙƒÙ„Ù…Ø§Øª ÙƒØ§Ù…Ù„Ø©
                if dept_keywords_set.issubset(all_dept_words):
                    match_found = True
                    logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©: {d.get('name', '')}")
            
            # Ø·Ø±ÙŠÙ‚Ø© 5: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† "ENT" ÙÙŠ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ (ÙƒÙƒÙ„Ù…Ø© ÙƒØ§Ù…Ù„Ø©)
            if not match_found:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ "ENT" Ø£Ùˆ "Ø§Ù„Ø£Ø°Ù† ÙˆØ§Ù„Ø£Ù†Ù ÙˆØ§Ù„Ø­Ù†Ø¬Ø±Ø©"
                if 'ent' in dept_normalized or 'Ø§Ø°Ù†' in dept_normalized or 'Ø§Ù†Ù' in dept_normalized or 'Ø­Ù†Ø¬Ø±Ø©' in dept_normalized:
                    # (Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† dentistry ØªÙ… Ø¨Ø§Ù„ÙØ¹Ù„ ÙÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø­Ù„Ù‚Ø©)
                    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† "ENT" ÙƒÙƒÙ„Ù…Ø© ÙƒØ§Ù…Ù„Ø© (ÙˆÙ„ÙŠØ³ Ø¬Ø²Ø¡Ø§Ù‹ Ù…Ù† ÙƒÙ„Ù…Ø© Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ "dentistry")
                    ent_pattern = r'\bent\b'
                    if (re.search(ent_pattern, doc_dept_en.lower(), re.IGNORECASE) or 
                        re.search(ent_pattern, all_dept_text.lower(), re.IGNORECASE) or
                        'head & neck' in doc_dept_en.lower() or 
                        'head and neck' in doc_dept_en.lower() or
                        'otolaryngology' in doc_dept_en.lower()):
                        match_found = True
                        logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ ENT Ù…Ø¨Ø§Ø´Ø±: {d.get('name', '')} (dept_en: {doc_dept_en[:50]})")
            
            # Ø·Ø±ÙŠÙ‚Ø© 6: fuzzy matching (Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹ - threshold Ø¹Ø§Ù„ÙŠ)
            if not match_found:
                for field in [doc_dept, doc_dept_ar, doc_dept_en]:
                    if field and len(field) > 3:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø­Ù‚ÙˆÙ„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                        similarity = fuzz.ratio(dept_normalized, field)
                        if similarity > 90:  # threshold Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ Ù„Ù„Ø¯Ù‚Ø©
                            match_found = True
                            logger.info(f"      âœ… ØªØ·Ø§Ø¨Ù‚ fuzzy: {d.get('name', '')} (similarity: {similarity})")
                            break
            
            if match_found:
                filtered_by_dept.append(d)
        
        # âœ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ø§Ù„Ù‚Ø³Ù… Ù…Ø¨Ø§Ø´Ø±Ø©
        filtered_by_dept = _remove_duplicate_doctors(filtered_by_dept)
        
        filtered = filtered_by_dept
        logger.info(f"   Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ø§Ù„Ù‚Ø³Ù…: {len(filtered)} Ø·Ø¨ÙŠØ¨")
        
        # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¯Ù‚Ø© Ø§Ù„ÙÙ„ØªØ±Ø© - Ø¹Ø±Ø¶ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ÙÙ„ØªØ±ÙŠÙ†
        if filtered:
            logger.info(f"   ğŸ“‹ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ÙÙ„ØªØ±ÙŠÙ†:")
            for doc in filtered[:5]:  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
                logger.info(f"      - {doc.get('name', '')} | {doc.get('department_ar', '')} | {doc.get('department_en', '')}")
        
        # âœ… Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø£Ø·Ø¨Ø§Ø¡ Ù„Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ØŒ Ù„Ø§ Ù†Ø¹Ø±Ø¶ ÙƒÙ„ Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
        # Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„ÙƒØŒ Ù†Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ©
        if len(filtered) == 0 and hospital:
            logger.warning(f"   âš ï¸ Ù„Ù… ÙŠÙÙˆØ¬Ø¯ Ø£Ø·Ø¨Ø§Ø¡ Ù…Ø·Ø§Ø¨Ù‚ÙŠÙ† Ù„Ù„Ù‚Ø³Ù… '{department}' ÙÙŠ Ù…Ø³ØªØ´ÙÙ‰ '{hospital}'")
            # Ù„Ø§ Ù†Ø¹Ø±Ø¶ Ø£Ø·Ø¨Ø§Ø¡ Ù…Ù† Ø£Ù‚Ø³Ø§Ù… Ø£Ø®Ø±Ù‰ - Ù†ØªØ±Ùƒ filtered ÙØ§Ø±ØºØ§Ù‹
            # Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø·Ø¨Ø§Ø¡" ÙÙŠ unified_inline_query
    
    # âœ… ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„ØªØ®ØµØµ (Ø¨Ø§Ø·Ù†ÙŠ/Ø¬Ø±Ø§Ø­ÙŠ)
    if specialty_type and filtered:
        specialty_type_lower = specialty_type.lower().strip()
        filtered_by_specialty = []
        
        for d in filtered:
            doc_dept = normalize_text(d.get('department', ''))
            doc_dept_ar = normalize_text(d.get('department_ar', ''))
            doc_dept_en = normalize_text(d.get('department_en', ''))
            
            all_dept_text = f"{doc_dept} {doc_dept_ar} {doc_dept_en}".lower()
            
            is_surgical = any(keyword in all_dept_text for keyword in [
                'Ø¬Ø±Ø§Ø­Ø©', 'surgery', 'surgical', 'operation', 'operative'
            ])
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¨Ø§Ø·Ù†ÙŠ: ÙŠØ¬Ø¨ Ø£Ù† Ù„Ø§ ÙŠÙƒÙˆÙ† Ø¬Ø±Ø§Ø­ÙŠ Ø£ÙˆÙ„Ø§Ù‹
            is_medical = False
            if not is_surgical:
                # ÙƒÙ„Ù…Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø¨Ø§Ø·Ù†ÙŠ
                medical_keywords = [
                    'Ø¨Ø§Ø·Ù†ÙŠ', 'medical', 'medicine', 'internal', 'physician', 
                    'cardiology', 'gastroenterology', 'neurology', 'nephrology', 
                    'pulmonology', 'endocrinology', 'hematology', 'rheumatology', 
                    'dermatology', 'psychiatry', 'pediatrics', 'geriatrics',
                    'allergy', 'immunology', 'infectious', 'critical care'
                ]
                is_medical = any(keyword in all_dept_text for keyword in medical_keywords)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„ØªØ±Ø©
            if specialty_type_lower == 'surgical' and is_surgical:
                filtered_by_specialty.append(d)
            elif specialty_type_lower == 'medical' and is_medical:
                filtered_by_specialty.append(d)
            elif specialty_type_lower not in ['medical', 'surgical']:
                # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†ÙˆØ¹ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØŒ Ù†Ø¹Ø±Ø¶ Ø§Ù„Ø¬Ù…ÙŠØ¹
                filtered_by_specialty.append(d)
        
        filtered = filtered_by_specialty
        logger.info(f"   Ø¨Ø¹Ø¯ ÙÙ„ØªØ±Ø© Ù†ÙˆØ¹ Ø§Ù„ØªØ®ØµØµ ({specialty_type}): {len(filtered)} Ø·Ø¨ÙŠØ¨")
    
    # âœ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
    filtered = _remove_duplicate_doctors(filtered)
    
    # Ø¥Ø°Ø§ Ù„Ø§ ÙŠÙˆØ¬Ø¯ queryØŒ Ø£Ø±Ø¬Ø¹ ÙƒÙ„ Ø§Ù„Ù…ÙÙ„ØªØ±ÙŠÙ† Ù…Ø±ØªØ¨ÙŠÙ† Ø£Ø¨Ø¬Ø¯ÙŠØ§Ù‹
    if not query or len(query.strip()) == 0:
        # âœ… ØªØ±ØªÙŠØ¨ Ø£Ø¨Ø¬Ø¯ÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø§Ø³Ù…
        filtered_sorted = sorted(filtered, key=lambda x: normalize_text(x.get('name', '')))
        logger.info(f"   â†’ Ø¥Ø±Ø¬Ø§Ø¹ {min(len(filtered_sorted), limit)} Ø·Ø¨ÙŠØ¨ Ù…Ø±ØªØ¨ Ø£Ø¨Ø¬Ø¯ÙŠØ§Ù‹")
        return filtered_sorted[:limit]
    
    # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…Ø¹ Ù†Ø¸Ø§Ù… ØªØ±ØªÙŠØ¨ Ù…ØªÙ‚Ø¯Ù…
    query_normalized = normalize_text(query)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø¨Ø­Ø« Ù…Ø¹ Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ù…ØªÙ‚Ø¯Ù…Ø©
    search_items = []
    for doc in filtered:
        name = doc.get('name', '')
        name_normalized = normalize_text(name)
        doc_hospital = normalize_text(doc.get('hospital', ''))
        doc_dept_ar = normalize_text(doc.get('department_ar', ''))
        doc_dept_en = normalize_text(doc.get('department_en', ''))
        
        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ù…ØªÙ‚Ø¯Ù…Ø©
        advanced_score = 0
        
        # 1. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø§Ø³Ù… (0-100 Ù†Ù‚Ø·Ø©)
        name_score = fuzz.WRatio(query_normalized, name_normalized)
        advanced_score += name_score * 0.5  # 50% Ù…Ù† Ø§Ù„Ù†Ù‚Ø§Ø·
        
        # 2. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ø¥Ø¶Ø§ÙÙŠ +20 Ù†Ù‚Ø·Ø©)
        if name_normalized.startswith(query_normalized):
            advanced_score += 20
        
        # 3. ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ (Ø¥Ø¶Ø§ÙÙŠ +30 Ù†Ù‚Ø·Ø©)
        if query_normalized == name_normalized:
            advanced_score += 30
        
        # 4. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ (Ø¥Ø¶Ø§ÙÙŠ +15 Ù†Ù‚Ø·Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø­Ø¯Ø¯)
        if hospital:
            hospital_normalized = normalize_text(hospital)
            hospital_match = fuzz.ratio(hospital_normalized, doc_hospital)
            if hospital_match > 80:
                advanced_score += 15
        
        # 5. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù… (Ø¥Ø¶Ø§ÙÙŠ +20 Ù†Ù‚Ø·Ø© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø­Ø¯Ø¯)
        if department:
            dept_normalized = normalize_text(department)
            dept_ar_normalized = normalize_text(department.split('|')[0] if '|' in department else department)
            dept_en_normalized = normalize_text(department.split('|')[1] if '|' in department and len(department.split('|')) > 1 else '')
            
            # ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ
            if dept_ar_normalized and doc_dept_ar:
                dept_ar_match = fuzz.ratio(dept_ar_normalized, doc_dept_ar)
                if dept_ar_match > 70:
                    advanced_score += 20
            
            # ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ
            if dept_en_normalized and doc_dept_en:
                dept_en_match = fuzz.ratio(dept_en_normalized, doc_dept_en)
                if dept_en_match > 70:
                    advanced_score += 20
        
        # 6. ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© (Ø¥Ø¶Ø§ÙÙŠ +10 Ù†Ù‚Ø·Ø©)
        query_words = query_normalized.split()
        if len(query_words) > 1:
            matched_words = sum(1 for word in query_words if word in name_normalized)
            if matched_words == len(query_words):
                advanced_score += 10
        
        search_items.append({
            'doctor': doc,
            'search_text': name_normalized,
            'display_name': name,
            'advanced_score': advanced_score,
            'name_score': name_score
        })
    
    # Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidFuzz
    if not search_items:
        logger.warning("   âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ± Ù„Ù„Ø¨Ø­Ø«")
        return []
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… RapidFuzz Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚Ø§Øª Ø£ÙˆÙ„ÙŠØ©
    matches = process.extract(
        query_normalized,
        [item['search_text'] for item in search_items],
        scorer=fuzz.WRatio,
        limit=min(len(search_items), limit * 3),  # Ø£Ø®Ø° Ø£ÙƒØ«Ø± Ù…Ù† limit Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        score_cutoff=30  # Ø­Ø¯ Ø£Ø¯Ù†Ù‰ Ù„Ù„ØªØ·Ø§Ø¨Ù‚
    )
    
    logger.info(f"   â†’ ÙˆÙØ¬Ø¯ {len(matches)} ØªØ·Ø§Ø¨Ù‚ Ø£ÙˆÙ„ÙŠ")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
    results = []
    for match_text, fuzz_score, idx in matches:
        item = search_items[idx]
        doctor = item['doctor']
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø·: 40% RapidFuzz + 60% Advanced Score
        final_score = (fuzz_score * 0.4) + (item['advanced_score'] * 0.6)
        
        results.append({
            'name': doctor.get('name', ''),
            'hospital': doctor.get('hospital', ''),
            'department': doctor.get('department', ''),
            'department_ar': doctor.get('department_ar', ''),
            'department_en': doctor.get('department_en', ''),
            'score': final_score,
            'fuzz_score': fuzz_score,
            'advanced_score': item['advanced_score']
        })
    
    # âœ… ØªØ±ØªÙŠØ¨ Ù…ØªÙ‚Ø¯Ù…: Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©ØŒ Ø«Ù… ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù…ØŒ Ø«Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ØŒ Ø«Ù… Ø£Ø¨Ø¬Ø¯ÙŠØ§Ù‹
    def sort_key(x):
        score = -x['score']  # Ø³Ø§Ù„Ø¨ Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØªÙ†Ø§Ø²Ù„ÙŠ
        
        # Ø£ÙˆÙ„ÙˆÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© Ù„Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¯Ù‚ÙŠÙ‚
        dept_bonus = 0
        if department:
            dept_normalized = normalize_text(department)
            doc_dept = normalize_text(x.get('department', ''))
            if dept_normalized in doc_dept or doc_dept in dept_normalized:
                dept_bonus = -50  # Ø¥Ø¶Ø§ÙØ© Ø£ÙˆÙ„ÙˆÙŠØ© (Ø³Ø§Ù„Ø¨Ø© Ù„Ø£Ù†Ù†Ø§ Ù†Ø±ØªØ¨ ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹)
        
        hospital_bonus = 0
        if hospital:
            hospital_normalized = normalize_text(hospital)
            doc_hospital = normalize_text(x.get('hospital', ''))
            if hospital_normalized in doc_hospital or doc_hospital in hospital_normalized:
                hospital_bonus = -30
        
        # ØªØ±ØªÙŠØ¨ Ø£Ø¨Ø¬Ø¯ÙŠ ÙƒØ­Ù„ Ø£Ø®ÙŠØ±
        name_sort = normalize_text(x.get('name', ''))
        
        return (score + dept_bonus + hospital_bonus, name_sort)
    
    results_sorted = sorted(results, key=sort_key)
    
    logger.info(f"   â†’ ØªÙ… ØªØ±ØªÙŠØ¨ {len(results_sorted)} Ù†ØªÙŠØ¬Ø©")
    
    # âœ… ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹)
    if OPENAI_AVAILABLE and len(results_sorted) > 3:
        try:
            results_sorted = _ai_enhanced_ranking(
                results_sorted, 
                query, 
                hospital, 
                department
            )
            logger.info("   âœ… ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI")
        except Exception as e:
            logger.warning(f"   âš ï¸ ÙØ´Ù„ ØªØ­Ø³ÙŠÙ† AI Ø§Ù„ØªØ±ØªÙŠØ¨: {e}")
    
    # âœ… ØªØ­Ù‚Ù‚ Ù†Ù‡Ø§Ø¦ÙŠ ØµØ§Ø±Ù…: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙˆØ§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ÙŠÙ† ÙÙ‚Ø·
    if hospital or department:
        final_results = []
        hospital_normalized = normalize_text(hospital) if hospital else None
        dept_normalized = normalize_text(department) if department else None
        
        for result in results_sorted:
            doc_hospital = normalize_text(result.get('hospital', ''))
            doc_dept_ar = normalize_text(result.get('department_ar', ''))
            doc_dept_en = normalize_text(result.get('department_en', ''))
            doc_dept = normalize_text(result.get('department', ''))
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø­Ø¯Ø¯Ø§Ù‹)
            hospital_match = True
            if hospital_normalized:
                hospital_match_ratio = fuzz.ratio(hospital_normalized, doc_hospital)
                # Ù‚Ø¨ÙˆÙ„ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ØŒ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ø²Ø¦ÙŠØŒ Ø£Ùˆ fuzzy Ø¹Ø§Ù„ÙŠ
                hospital_match = (
                    hospital_normalized == doc_hospital or 
                    hospital_normalized in doc_hospital or 
                    doc_hospital in hospital_normalized or
                    hospital_match_ratio >= 90
                )
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚Ø³Ù… (Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø­Ø¯Ø¯Ø§Ù‹)
            dept_match = True
            if dept_normalized:
                all_dept_text = f"{doc_dept} {doc_dept_ar} {doc_dept_en}".lower()
                # ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù‚Ø³Ù…
                dept_match = (
                    dept_normalized in all_dept_text or
                    all_dept_text in dept_normalized or
                    fuzz.ratio(dept_normalized, all_dept_text) >= 90
                )
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙ‚Ø· Ø¥Ø°Ø§ ØªØ·Ø§Ø¨Ù‚Øª Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰ ÙˆØ§Ù„Ù‚Ø³Ù… Ø§Ù„Ù…Ø­Ø¯Ø¯ÙŠÙ†
            if hospital_match and dept_match:
                final_results.append(result)
            else:
                logger.debug(f"   âŒ ØªÙ… Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ {result.get('name', '')} - Ù„Ø§ ÙŠØ·Ø§Ø¨Ù‚ Ø§Ù„ÙÙ„ØªØ±Ø© Ø§Ù„ØµØ§Ø±Ù…Ø© (hospital_match={hospital_match}, dept_match={dept_match})")
        
        results_sorted = final_results
        logger.info(f"   âœ… Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„ØµØ§Ø±Ù…: {len(results_sorted)} Ø·Ø¨ÙŠØ¨")
    
    # Ø¥Ø±Ø¬Ø§Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
    return results_sorted[:limit]


def _ai_enhanced_ranking(
    results: List[Dict], 
    query: str, 
    hospital: Optional[str] = None, 
    department: Optional[str] = None
) -> List[Dict]:
    """
    ØªØ­Ø³ÙŠÙ† ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AI
    
    ÙŠØ¹Ø·ÙŠ Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ø¨Ø§Ù„Ø¨Ø­Ø«
    """
    if not results or len(results) <= 3:
        return results
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ prompt Ù„Ù„Ù€ AI
        context = f"Query: {query or 'None'}"
        if hospital:
            context += f" | Hospital: {hospital}"
        if department:
            context += f" | Department: {department}"
        
        # Ø£Ø®Ø° Ø£ÙˆÙ„ 10 Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø· Ù„Ù„ØªØ­Ù„ÙŠÙ„ (Ù„Ù„ØªÙˆÙÙŠØ±)
        top_results = results[:10]
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ù„Ù„ØªØ­Ù„ÙŠÙ„
        doctors_list = []
        for idx, doc in enumerate(top_results):
            doctors_list.append({
                'index': idx,
                'name': doc.get('name', ''),
                'hospital': doc.get('hospital', ''),
                'department': doc.get('department_ar', '') or doc.get('department_en', '')
            })
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ø¸Ø§Ù… ØªØ±ØªÙŠØ¨ Ù…Ø­Ù„ÙŠ Ø°ÙƒÙŠ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† AI (Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©)
        # ÙŠÙ…ÙƒÙ† ØªÙØ¹ÙŠÙ„ AI Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        
        # ØªØ±ØªÙŠØ¨ Ø¥Ø¶Ø§ÙÙŠ Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©:
        # 1. ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„ ÙÙŠ Ø§Ù„Ø§Ø³Ù…
        # 2. ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
        # 3. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù…
        # 4. ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
        
        def calculate_relevance_score(doc: Dict) -> float:
            score = doc.get('score', 0)
            
            name = normalize_text(doc.get('name', ''))
            query_norm = normalize_text(query) if query else ''
            
            # ØªØ·Ø§Ø¨Ù‚ ÙƒØ§Ù…Ù„
            if query_norm and query_norm == name:
                score += 50
            
            # ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
            elif query_norm and name.startswith(query_norm):
                score += 30
            
            # ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù‚Ø³Ù…
            if department:
                dept_norm = normalize_text(department)
                doc_dept = normalize_text(doc.get('department_ar', '') + ' ' + doc.get('department_en', ''))
                if dept_norm in doc_dept:
                    score += 20
            
            # ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
            if hospital:
                hosp_norm = normalize_text(hospital)
                doc_hosp = normalize_text(doc.get('hospital', ''))
                if hosp_norm in doc_hosp:
                    score += 15
            
            return score
        
        # Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
        for doc in top_results:
            doc['relevance_score'] = calculate_relevance_score(doc)
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        top_results_sorted = sorted(
            top_results, 
            key=lambda x: (-x.get('relevance_score', 0), normalize_text(x.get('name', '')))
        )
        
        # Ø¯Ù…Ø¬ Ù…Ø¹ Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        final_results = top_results_sorted + results[10:]
        
        return final_results
        
    except Exception as e:
        logger.warning(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±ØªÙŠØ¨: {e}")
        return results


def get_departments_for_hospital(hospital):
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù„Ù…Ø³ØªØ´ÙÙ‰ Ù…Ø¹ÙŠÙ†
    
    Args:
        hospital: Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„ÙØ±ÙŠØ¯Ø©
    """
    doctors = load_doctors()
    
    if not doctors or not hospital:
        return []
    
    hospital_normalized = normalize_text(hospital)
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
    hospital_doctors = [
        d for d in doctors
        if hospital_normalized in normalize_text(d.get('hospital', ''))
    ]
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„ÙØ±ÙŠØ¯Ø©
    departments = set()
    for doc in hospital_doctors:
        dept = doc.get('department', '').strip()
        if dept and dept not in ['Unknown', 'Not specified', 'General']:
            departments.add(dept)
    
    return sorted(list(departments))


def get_doctors_for_hospital_dept(hospital, department):
    """
    Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ÙƒÙ„ Ø£Ø·Ø¨Ø§Ø¡ Ù…Ø³ØªØ´ÙÙ‰ ÙˆÙ‚Ø³Ù… Ù…Ø¹ÙŠÙ†
    
    Args:
        hospital: Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ´ÙÙ‰
        department: Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù…
    
    Returns:
        Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡
    """
    return search_doctors(query="", hospital=hospital, department=department, limit=100)


# ØªÙ… ØªØ¹Ø·ÙŠÙ„ Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙŠØ¯ÙˆÙŠØ© - Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø© Ø«Ø§Ø¨ØªØ© ÙˆÙ†Ø¸ÙŠÙØ©


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

if __name__ == "__main__":
    print("="*70)
    print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ")
    print("="*70)
    
    # ØªØ­Ù…ÙŠÙ„
    doctors = load_doctors()
    print(f"\nâœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(doctors)} Ø·Ø¨ÙŠØ¨")
    
    # Ø§Ø®ØªØ¨Ø§Ø± 1: Ø¨Ø­Ø« Ø¹Ø§Ù…
    print("\n" + "="*70)
    print("Ø§Ø®ØªØ¨Ø§Ø± 1: Ø¨Ø­Ø« Ø¹Ø§Ù… Ø¹Ù† 'ahmed'")
    results = search_doctors("ahmed", limit=5)
    for r in results:
        print(f"  â€¢ {r['name']} | {r['hospital']} | {r['department']}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± 2: Ø¨Ø­Ø« Ø¨Ù…Ø³ØªØ´ÙÙ‰
    print("\n" + "="*70)
    print("Ø§Ø®ØªØ¨Ø§Ø± 2: Ø¨Ø­Ø« Ø¹Ù† 'kumar' ÙÙŠ Sakra")
    results = search_doctors("kumar", hospital="Sakra", limit=5)
    for r in results:
        print(f"  â€¢ {r['name']} | {r['hospital']} | {r['department']}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± 3: Ø¨Ø­Ø« Ø¨Ù…Ø³ØªØ´ÙÙ‰ ÙˆÙ‚Ø³Ù…
    print("\n" + "="*70)
    print("Ø§Ø®ØªØ¨Ø§Ø± 3: Ø¨Ø­Ø« Ø¹Ù† 'raj' ÙÙŠ Sakra - Cardiology")
    results = search_doctors("raj", hospital="Sakra", department="Cardio", limit=5)
    for r in results:
        print(f"  â€¢ {r['name']} | {r['hospital']} | {r['department']}")
    
    # Ø§Ø®ØªØ¨Ø§Ø± 4: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ù…Ø³ØªØ´ÙÙ‰
    print("\n" + "="*70)
    print("Ø§Ø®ØªØ¨Ø§Ø± 4: Ø£Ù‚Ø³Ø§Ù… Sakra World Hospital")
    depts = get_departments_for_hospital("Sakra World Hospital")
    print(f"  ÙˆÙØ¬Ø¯ {len(depts)} Ù‚Ø³Ù…")
    for dept in depts[:10]:
        print(f"  â€¢ {dept}")
    
    print("\n" + "="*70)

